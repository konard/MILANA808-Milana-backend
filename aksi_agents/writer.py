"""
AI-Writer Agent
Generates issue and PR content with multiple variants
"""

from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import hashlib
import time


@dataclass
class WrittenContent:
    """Content variant generated by AI-Writer"""
    variant_id: str
    title: str
    body: str
    labels: List[str]
    eqs_score: float  # EQS: meaning, logic, uniqueness
    metadata: Dict[str, Any]


class AIWriter:
    """
    AI-Writer Agent
    - Generates multiple variants of issues/PRs
    - Ranks variants by EQS metrics
    - Adapts to repository style
    """

    def __init__(self, logger=None):
        self.logger = logger

    def generate_issue_variants(
        self,
        topic: str,
        context: Dict[str, Any],
        num_variants: int = 3
    ) -> List[WrittenContent]:
        """Generate multiple variants of an issue"""
        if self.logger:
            self.logger.log("writer_generate_issues", {
                "topic": topic,
                "num_variants": num_variants
            })

        variants = []
        for i in range(num_variants):
            variant = self._create_issue_variant(topic, context, i)
            variants.append(variant)

        # Sort by EQS score
        variants.sort(key=lambda x: x.eqs_score, reverse=True)

        if self.logger:
            self.logger.log("writer_generated", {
                "topic": topic,
                "variants": len(variants),
                "top_score": variants[0].eqs_score if variants else 0
            })

        return variants

    def _create_issue_variant(self, topic: str, context: Dict[str, Any], variant_num: int) -> WrittenContent:
        """Create a single issue variant"""
        variant_id = hashlib.md5(f"{topic}_{variant_num}_{time.time()}".encode()).hexdigest()[:8]

        # Generate title variations
        titles = [
            f"feat: {topic}",
            f"enhancement: Implement {topic}",
            f"feature: Add {topic} functionality"
        ]
        title = titles[variant_num % len(titles)]

        # Generate body variations
        bodies = [
            self._generate_detailed_body(topic, context),
            self._generate_concise_body(topic, context),
            self._generate_technical_body(topic, context)
        ]
        body = bodies[variant_num % len(bodies)]

        # Determine labels
        labels = self._suggest_labels(topic, context)

        # Calculate EQS score
        eqs_score = self._calculate_eqs_score(title, body, context)

        return WrittenContent(
            variant_id=variant_id,
            title=title,
            body=body,
            labels=labels,
            eqs_score=eqs_score,
            metadata={
                "variant_num": variant_num,
                "created_at": int(time.time()),
                "topic": topic
            }
        )

    def _generate_detailed_body(self, topic: str, context: Dict[str, Any]) -> str:
        """Generate detailed issue body"""
        return f"""## Overview
Implement {topic} to enhance the system capabilities.

## Problem Statement
{context.get('problem', 'Current implementation needs improvement')}

## Proposed Solution
{context.get('solution', f'Add comprehensive {topic} support')}

## Benefits
- Improved functionality
- Better user experience
- Enhanced maintainability

## Implementation Notes
{context.get('notes', 'Follow existing code patterns and add tests')}

## Related
{context.get('related', 'See similar implementations in the codebase')}
"""

    def _generate_concise_body(self, topic: str, context: Dict[str, Any]) -> str:
        """Generate concise issue body"""
        return f"""**Goal:** {topic}

**Why:** {context.get('problem', 'To improve system functionality')}

**How:** {context.get('solution', f'Implement {topic} feature')}

**Checklist:**
- [ ] Design implementation
- [ ] Write code
- [ ] Add tests
- [ ] Update docs
"""

    def _generate_technical_body(self, topic: str, context: Dict[str, Any]) -> str:
        """Generate technical issue body"""
        return f"""### Technical Specification: {topic}

**Scope:** {context.get('scope', 'Backend implementation')}

**Requirements:**
1. {context.get('req1', 'Functional requirements')}
2. {context.get('req2', 'Performance requirements')}
3. {context.get('req3', 'Testing requirements')}

**Architecture:**
```
{context.get('architecture', 'Module -> Service -> API')}
```

**Dependencies:** {context.get('dependencies', 'None')}

**Testing Strategy:** {context.get('testing', 'Unit + Integration tests')}
"""

    def _suggest_labels(self, topic: str, context: Dict[str, Any]) -> List[str]:
        """Suggest appropriate labels for the issue"""
        labels = []

        # Base labels
        if "feat" in topic.lower() or "feature" in topic.lower():
            labels.append("enhancement")
        if "fix" in topic.lower() or "bug" in topic.lower():
            labels.append("bug")
        if "test" in topic.lower():
            labels.append("testing")
        if "doc" in topic.lower():
            labels.append("documentation")

        # Context-based labels
        if context.get("priority") == "high":
            labels.append("priority")
        if context.get("automation"):
            labels.append("automation")

        # Default
        if not labels:
            labels.append("enhancement")

        return labels

    def _calculate_eqs_score(self, title: str, body: str, context: Dict[str, Any]) -> float:
        """
        Calculate EQS (Evaluation Quality Score)
        Based on: meaning, logic, uniqueness
        """
        score = 0.0

        # Meaning: Does it convey clear intent?
        if len(title) > 10 and len(body) > 50:
            score += 0.3
        if "##" in body or "**" in body:  # Has structure
            score += 0.1

        # Logic: Is it well-structured?
        if "Problem" in body or "Solution" in body:
            score += 0.2
        if "checklist" in body.lower() or "[ ]" in body:
            score += 0.15

        # Uniqueness: Does it provide specific value?
        if context.get("patterns"):
            score += 0.15
        if len(body) > 200:  # Detailed description
            score += 0.1

        # Normalize to 0-1 range
        return min(score, 1.0)

    def generate_pr_description(
        self,
        issue_number: int,
        changes: List[str],
        context: Dict[str, Any]
    ) -> str:
        """Generate PR description based on issue and changes"""
        if self.logger:
            self.logger.log("writer_generate_pr", {
                "issue": issue_number,
                "changes": len(changes)
            })

        description = f"""## Summary
Fixes #{issue_number}

## Changes
{chr(10).join(f'- {change}' for change in changes)}

## Testing
{context.get('testing', '- [ ] Unit tests added\n- [ ] Manual testing completed')}

## Checklist
- [ ] Code follows project style
- [ ] Tests passing
- [ ] Documentation updated
"""
        return description
